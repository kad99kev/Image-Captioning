{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "several-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.dataset import FeatureCaptionDataset\n",
    "from utils.models import CNNEncoder, RNNDecoder\n",
    "from utils.helpers import get_tokenizer_vocab, collate_fn_pad\n",
    "from utils.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "synthetic-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "objective-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH + \"captions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cloudy-lexington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scheduled-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, vocab = get_tokenizer_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "historical-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FeatureCaptionDataset(PATH + \"embeddings/\", df, tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "legendary-exercise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2529"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, collate_fn=collate_fn_pad)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "collect-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim, decoder_dim, attention_dim, embed_dim, vocab_size = 512, 256, 128, 64, len(vocab)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "norwegian-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = 0\n",
    "# for i in tqdm(range(len(dataset))):\n",
    "#     _, cap = dataset[i]\n",
    "#     cap_len = len(cap)\n",
    "#     if cap_len > max_len:\n",
    "#         max_len = cap_len\n",
    "max_len = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "arctic-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNNEncoder(1536, encoder_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fifteen-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RNNDecoder(encoder_dim=encoder_dim, \n",
    "                     decoder_dim=decoder_dim, \n",
    "                     attention_dim=attention_dim, \n",
    "                     embedding_dim=embed_dim, \n",
    "                     vocab_size=vocab_size\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee069ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()))\n",
    "ce_loss = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "brown-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(encoder, decoder, optimizer, ce_loss, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd23e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f603c87df44e0db5506c86e89d0fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Epoch 1/2 **********\n",
      "Average Batch 0 Loss: 5.746832142705503\n",
      "Average Batch 100 Loss: 2.8956720352172853\n",
      "Average Batch 200 Loss: 3.210963399786698\n",
      "Average Batch 300 Loss: 3.152499198913574\n",
      "Average Batch 400 Loss: 2.514632138338956\n",
      "Average Batch 500 Loss: 2.224023183186849\n",
      "Average Batch 600 Loss: 2.1356488863627114\n",
      "Average Batch 700 Loss: 2.0676881408691408\n",
      "Average Batch 800 Loss: 2.8970696131388345\n",
      "Average Batch 900 Loss: 2.2200374603271484\n",
      "Average Batch 1000 Loss: 2.3499270629882814\n",
      "Average Batch 1100 Loss: 2.323818418714735\n",
      "Average Batch 1200 Loss: 2.312026575991982\n",
      "Average Batch 1300 Loss: 2.5658302307128906\n",
      "Average Batch 1400 Loss: 1.861580335176908\n",
      "Average Batch 1500 Loss: 2.323731952243381\n",
      "Average Batch 1600 Loss: 2.0501084899902344\n",
      "Average Batch 1700 Loss: 2.1074271731906467\n",
      "Average Batch 1800 Loss: 2.6225140220240544\n",
      "Average Batch 1900 Loss: 1.724917449951172\n",
      "Average Batch 2000 Loss: 1.9071004867553711\n",
      "Average Batch 2100 Loss: 1.8728458086649578\n",
      "Average Batch 2200 Loss: 1.7195382118225098\n",
      "Average Batch 2300 Loss: 1.8670672541079314\n",
      "Average Batch 2400 Loss: 1.7670371444137007\n",
      "Average Batch 2500 Loss: 1.8727190229627821\n",
      "Total Loss: 2.341764\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataloader, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-easter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
