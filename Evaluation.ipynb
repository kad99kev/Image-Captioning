{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "growing-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.dataset import FeatureCaptionDataset\n",
    "from utils.models import CNNEncoder, RNNDecoder\n",
    "from utils.helpers import get_tokenizer_vocab, collate_fn_pad\n",
    "from utils.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "honey-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brilliant-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH + \"captions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "offshore-wrapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "median-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, vocab = get_tokenizer_vocab(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "latest-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FeatureCaptionDataset(PATH + \"embeddings/\", df, tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "periodic-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dim, decoder_dim, attention_dim, embed_dim, vocab_size, max_len = 512, 256, 128, 64, len(vocab), 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "scientific-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNNEncoder(1536, encoder_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "different-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RNNDecoder(encoder_dim=encoder_dim, \n",
    "                     decoder_dim=decoder_dim, \n",
    "                     attention_dim=attention_dim, \n",
    "                     embedding_dim=embed_dim, \n",
    "                     vocab_size=vocab_size\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "allied-procedure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNDecoder(\n",
       "  (embedding): Embedding(9213, 64)\n",
       "  (gru): GRU(576, 256, batch_first=True)\n",
       "  (fc_1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc_2): Linear(in_features=256, out_features=9213, bias=True)\n",
       "  (attn): Attention(\n",
       "    (encoder_attn): Linear(in_features=512, out_features=128, bias=False)\n",
       "    (decoder_attn): Linear(in_features=256, out_features=128, bias=False)\n",
       "    (value): Linear(in_features=128, out_features=1, bias=False)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load(\"models/encoder.pt\"))\n",
    "encoder.eval()\n",
    "\n",
    "decoder.load_state_dict(torch.load(\"models/decoder.pt\"))\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "gothic-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cap = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "certain-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-ca9c518fe309>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  enc = encoder(torch.tensor(torch.tensor(img).unsqueeze(0)))\n"
     ]
    }
   ],
   "source": [
    "enc = encoder(torch.tensor(torch.tensor(img).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "brutal-halifax",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'encoder_output' and 'hidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a6a67236fa60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'encoder_output' and 'hidden'"
     ]
    }
   ],
   "source": [
    "dec = decoder(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-seeker",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
